{"cells":[{"cell_type":"markdown","source":["## Task 1: Spark SQL (15m)"],"metadata":{"id":"yvjBmGBAxnQc","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36b565b0-dc53-4172-b646-4c82e1c472be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"MkbrHZYEw5Cr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d54257-dc20-4174-aa40-84e1f6abc56f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sales_file_location = \"/FileStore/tables/Sales_table.csv\"\nproducts_file_location = \"/FileStore/tables/Products_table.csv\"\nsellers_file_location = \"/FileStore/tables/Sellers_table.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nproducts_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(products_file_location)\n\nsales_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sales_file_location)\n\nsellers_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sellers_file_location)"],"metadata":{"id":"2luSAeOXxBiQ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5607f10e-0a58-4330-bbeb-fa1d6863efb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["products_table.show()\nsales_table.show()\nsellers_table.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30a28559-bd17-4518-9c89-c44b1d95b6c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+-----+\n|product_id|product_name|price|\n+----------+------------+-----+\n|         1|   product_1|   24|\n|         2|   product_2|  173|\n|         3|   product_3|  147|\n|         4|   product_4|  116|\n|         5|   product_5|   13|\n|         6|   product_6|  146|\n|         7|   product_7|  121|\n|         8|   product_8|   21|\n|         9|   product_9|   70|\n|        10|  product_10|   28|\n|        11|  product_11|  113|\n|        12|  product_12|  105|\n|        13|  product_13|  133|\n|        14|  product_14|   75|\n|        15|  product_15|  113|\n|        16|  product_16|   98|\n|        17|  product_17|   17|\n|        18|  product_18|  177|\n|        19|  product_19|   29|\n|        20|  product_20|  163|\n+----------+------------+-----+\nonly showing top 20 rows\n\n+--------+----------+---------+-----------------+\n|order_id|product_id|seller_id|num_of_items_sold|\n+--------+----------+---------+-----------------+\n|       1|      1841|    35172|              432|\n|       2|     14496|     6362|              854|\n|       3|     76927|    23608|              850|\n|       4|     17982|    21413|              392|\n|       5|     74388|    23888|              227|\n|       6|     22330|     1630|               35|\n|       7|     20303|    34610|              115|\n|       8|     83283|     5818|              689|\n|       9|     24399|    13860|              263|\n|      10|     47264|    22833|              747|\n|      11|     34571|    48658|              278|\n|      12|     86135|     1543|              988|\n|      13|     13235|     4478|              523|\n|      14|     11279|    48171|              398|\n|      15|     35388|     9663|              787|\n|      16|     94026|     9361|              167|\n|      17|     22776|    10269|              241|\n|      18|     68544|    14117|              682|\n|      19|     55060|     8801|              566|\n|      20|      8622|    44907|               74|\n+--------+----------+---------+-----------------+\nonly showing top 20 rows\n\n+---------+-----------+------+\n|seller_id|seller_name|rating|\n+---------+-----------+------+\n|        1|   seller_1|     2|\n|        2|   seller_2|     5|\n|        3|   seller_3|     1|\n|        4|   seller_4|     5|\n|        5|   seller_5|     2|\n|        6|   seller_6|     1|\n|        7|   seller_7|     4|\n|        8|   seller_8|     1|\n|        9|   seller_9|     5|\n|       10|  seller_10|     2|\n|       11|  seller_11|     2|\n|       12|  seller_12|     3|\n|       13|  seller_13|     3|\n|       14|  seller_14|     1|\n|       15|  seller_15|     1|\n|       16|  seller_16|     2|\n|       17|  seller_17|     1|\n|       18|  seller_18|     3|\n|       19|  seller_19|     4|\n|       20|  seller_20|     1|\n+---------+-----------+------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["products_table.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"726ddc18-a98c-4ea4-aea8-168bd9b1cc8d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+-----+\n|product_id|product_name|price|\n+----------+------------+-----+\n|         1|   product_1|   24|\n|         2|   product_2|  173|\n|         3|   product_3|  147|\n|         4|   product_4|  116|\n|         5|   product_5|   13|\n|         6|   product_6|  146|\n|         7|   product_7|  121|\n|         8|   product_8|   21|\n|         9|   product_9|   70|\n|        10|  product_10|   28|\n|        11|  product_11|  113|\n|        12|  product_12|  105|\n|        13|  product_13|  133|\n|        14|  product_14|   75|\n|        15|  product_15|  113|\n|        16|  product_16|   98|\n|        17|  product_17|   17|\n|        18|  product_18|  177|\n|        19|  product_19|   29|\n|        20|  product_20|  163|\n+----------+------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (a) Output the top 3 most popular products sold among all sellers [2m]\n# Your table should have 1 column(s): [product_name]\n# from pyspark.sql.functions import col\n\ntop_three_product_id_df = sales_table.groupBy(\"product_id\")\\\n    .count()\\\n    .orderBy(col(\"count\").desc(), col(\"product_id\").asc())\\\n    .limit(3)\\\n\ntop_three_product_name = products_table.join(top_three_product_id_df, \"product_id\")\\\n    .select(col(\"product_name\"))\\\n    .show()\n\n\n"],"metadata":{"id":"Ps_v7oTixnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fb33021-930c-4fa9-b595-4ed83c279ed4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n| product_8031|\n|product_22622|\n|product_99849|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (b) Find out the total sales of the products sold by sellers 1 to 10 and output the top most sold product [2m]\n# Your table should have 1 column(s): [product_name]\n\nsales_of_sellers_1_to_10 = sales_table.filter((col(\"seller_id\") >= 1) & (col(\"seller_id\") <= 10))\n\nsum_of_sales_for_products_sold_in_desc = sales_of_sellers_1_to_10.groupBy(\"product_id\")\\\n    .sum(\"num_of_items_sold\")\\\n    .orderBy(col(\"sum(num_of_items_sold)\").desc())\\\n    .limit(1)\n\nproduct_name_of_top_most_sold_product = sum_of_sales_for_products_sold_in_desc.join(products_table, \"product_id\")\\\n    .select(col(\"product_name\"))\\\n    .show()"],"metadata":{"id":"Ljmb_1OaxC8Q","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866983b3-8214-4740-8f4d-90e87d1db482","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n| product_name|\n+-------------+\n|product_36658|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n# Your table should have 1 column(s): [total_revenue]\n\nsales_of_seller_from_1_to_500 = sales_table.filter((col(\"seller_id\") >= 1) & (col(\"seller_id\") <= 500))\n\ncombined_sales_by_product_id_from_seller_1_to_500 = sales_of_seller_from_1_to_500.groupBy(\"product_id\")\\\n    .sum(\"num_of_items_sold\")\\\n\nsales_of_product_and_quantity = combined_sales_by_product_id_from_seller_1_to_500.join(products_table, \"product_id\")\\\n    .select(col(\"sum(num_of_items_sold)\"), col(\"price\"))\n\ndef calculate_revenue(row):\n    revenue = row[\"sum(num_of_items_sold)\"] * row[\"price\"]\n    return revenue\n\ntotal_revenue = sc.accumulator(0)\n\ndef update_total_revenue(total_revenue, row):\n    total_revenue.add(calculate_revenue(row))\n\nsales_of_product_and_quantity.foreach(lambda row: update_total_revenue(total_revenue, row))\n\nspark.createDataFrame([(total_revenue.value,)], ['total_revenue']).show()"],"metadata":{"id":"QtinRRycxDBS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa7bec8e-f93d-48ff-af38-d395c6fe7422","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|total_revenue|\n+-------------+\n|    160916699|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n# Your table should have 1 column(s): [product_name]\n# To get the full mark, your query should not run for more than 1 min\n\nsellers_with_rating_from_4 = sellers_table.filter(col(\"rating\") >= 4)\n\nsales_of_sellers_with_rating_from_4 = sellers_with_rating_from_4.join(sales_table, \"seller_id\")\\\n    .groupBy(\"seller_id\")\\\n    .sum(\"num_of_items_sold\")\n\nsellers_with_rating_from_4_and_sold_from_3000 = sales_of_sellers_with_rating_from_4.filter(col(\"sum(num_of_items_sold)\") >= 3000)\\\n    .select(col(\"seller_id\"))\n\nproducts_sold_by_filtered_sellers = sellers_with_rating_from_4_and_sold_from_3000.join(sales_table, \"seller_id\")\\\n    .select(col(\"product_id\"))\n\nproducts_sold_with_price = products_sold_by_filtered_sellers.join(products_table, \"product_id\")\\\n    .dropDuplicates([\"product_id\"])\\\n    .orderBy(col(\"price\").desc(), col(\"product_id\").asc())\\\n    .select(col(\"product_name\"))\\\n    .limit(10)\\\n    .show()"],"metadata":{"id":"jdG80LVMxnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59c00e0a-34de-4614-b783-71beb7503716","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+\n|product_name|\n+------------+\n| product_106|\n| product_117|\n| product_363|\n| product_712|\n| product_843|\n| product_897|\n| product_923|\n|product_1466|\n|product_1507|\n|product_1514|\n+------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Task 2: Spark ML (10m)"],"metadata":{"id":"4fziMyvTxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2551ab92-377c-4492-9d99-258610b143a1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"wtocOKQXxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ebc093d-9256-4e99-85d3-3d36b50a6053","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bank_train_location = \"/FileStore/tables/bank_train.csv\"\nbank_test_location = \"/FileStore/tables/bank_test.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nbank_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_train_location)\n\nbank_test = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_test_location)"],"metadata":{"id":"lQB18KhnxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2eee140e-773a-4e76-9f6c-40e809e136b0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set. \n* You can explore different methods to pre-process the data and select proper features\n* You can utilize different machine learning models and tune model hyperparameters\n* Present the final testing accuracy."],"metadata":{"id":"YTZevHlAxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98477bc0-fdf9-4585-8cf2-24b4b0ebc3f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# data preparation (4m)\n# Import necessary libraries\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline\n\n# Define the interested categorical and numerical columns\n# cat_cols = [\"job\", \"marital\", \"education\", \"housing\", \"loan\", \"poutcome\"]\n# num_cols = [\"age\", \"balance\"]\ncat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\nnum_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n\n# Convert categorical columns to numerical using OneHotEncoder\ncat_indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in cat_cols]\ncat_encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\") for col in cat_cols]\n\n# Assemble all features into a single vector column\nassembler = VectorAssembler(inputCols=num_cols + [col + \"_vec\" for col in cat_cols], outputCol=\"features\")"],"metadata":{"id":"iey06VQfxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e07aaf5a-6fb8-425a-a3c9-f52e04e49828","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n# model building (4m)\n# Define Random Forest Classifier model\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n\n# Define pipeline with all stages\npipeline = Pipeline(stages=cat_indexers + cat_encoders + [assembler, rf])\n\n# Fit pipeline on training data\nmodel = pipeline.fit(bank_train)"],"metadata":{"id":"PsIotb9ExnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a04b59e4-6197-451c-8071-52526a5a724f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model evaluation (2m)\n# Predict on test data\npredictions = model.transform(bank_test)\n\n# Evaluate the model using BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\nauc = evaluator.evaluate(predictions)\n\nprint(\"AUC on test data = {}\".format(auc))"],"metadata":{"id":"OC5ufJqAxnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e1c949-8291-45be-8872-c0310777c6fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["AUC on test data = 0.8990509841317872\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d26ebb86-cd1a-4bed-ab31-594bc6278f8f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"cs4225_a2_databricks_student_version","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3898635689612163}},"nbformat":4,"nbformat_minor":0}
